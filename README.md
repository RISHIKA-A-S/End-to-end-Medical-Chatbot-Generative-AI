# ğŸ§  End-to-End Medical Chatbot (Generative AI using Ollama & Pinecone)

An intelligent **medical question-answering chatbot** built with:

- ğŸ” Custom PDF knowledge ingestion
- ğŸ“š Semantic search using **Pinecone**
- ğŸ§  Local LLM inference via **Ollama** (e.g., LLaMA, Mistral)
- ğŸŒ Web interface powered by **Flask**

---

## ğŸš€ Features

- Ingests and indexes medical PDFs.
- Performs **semantic search** using **HuggingFace embeddings** + **Pinecone vector DB**.
- Generates answers using **local models** via **Ollama** (no API cost).
- Fully functional **Flask-based** web UI for easy interaction.
- Modular and scalable architecture for healthcare use cases.

---

## ğŸ› ï¸ Tech Stack

| Component          | Technology               |
|-------------------|--------------------------|
| Embeddings         | HuggingFace Transformers |
| Vector Store       | Pinecone                 |
| LLM Inference      | Ollama (LLaMA/Mistral)   |
| Backend            | Flask                    |
| PDF Parsing        | PyMuPDF / pdfminer.six   |

---

## ğŸ§ª Setup Instructions (Run Locally)

âœ… STEP 1: Clone the Repository

```bash
git clone https://github.com/<your-username>/End-to-End-Medical-Chatbot.git
cd End-to-End-Medical-Chatbot


âœ… STEP 2: Set up the Environment
bash
Copy
Edit
conda create -n medibot python=3.10 -y
conda activate medibot


âœ… STEP 3: Install Dependencies
bash
Copy
Edit
pip install -r requirements.txt


âœ… STEP 4: Set Environment Variables
Create a .env file in the root directory with:

env
Copy
Edit
PINECONE_API_KEY=your_pinecone_api_key
PINECONE_ENVIRONMENT=your_pinecone_env
PINECONE_INDEX_NAME=medical-chatbot-index


ğŸ§¾ Folder Structure
graphql
Copy
Edit
.
â”œâ”€â”€ app.py                  # Flask Web App
â”œâ”€â”€ ingest.py               # PDF Ingestion & Vectorization
â”œâ”€â”€ query.py                # Search & Response logic
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html          # Web UI
â”œâ”€â”€ static/
â”‚   â””â”€â”€ styles.css
â”œâ”€â”€ data/                   # Your PDF medical files
â”œâ”€â”€ .env                    # Environment variables
â”œâ”€â”€ requirements.txt


âš™ï¸ How It Works
PDFs are parsed and chunked into small contexts.

Each chunk is embedded using HuggingFace models.

Embeddings are stored in Pinecone vector DB.

When the user asks a question:

Top relevant chunks are retrieved from Pinecone.

Context + user query is passed to the Ollama LLM (like mistral, llama3).

A natural language answer is generated.

ğŸ“¸ Screenshot

ğŸ§ª Example Queries
"What are the symptoms of Type 2 diabetes?"

"Explain the side effects of Metformin."

"How does hypertension affect kidney function?"

ğŸ”’ Disclaimer
This project is for educational/demo purposes only. Not intended for actual clinical decision-making or diagnosis.

ğŸ“„ License
This project is licensed under the MIT License.

ğŸ¤ Contributing
PRs, feedback, and issues are welcome! Letâ€™s improve healthcare AI together.



